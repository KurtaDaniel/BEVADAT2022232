# -*- coding: utf-8 -*-
"""dense_native.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_O7FJmvzl4xhPu0KFUCWynQSVCqLkmeJ
"""

import numpy as np

class Dense:
    """A fully-connected NN layer.
    Parameters:
    -----------
    n_output: int
        The number of neurons in the layer.
    n_input: int
        The expected input shape of the layer. For dense layers a single digit specifying
        the number of features of the input. Must be specified if it is the first layer in
        the network.
    """

    def __init__(self, n_output, n_input=None):
        self.layer_input = None
        self.n_input = n_input
        self.n_output = n_output
        self.trainable = True
        self.W = None
        self.bias = None
        self.initialize()

    def initialize(self):
        # Initialize the weights
        np.random.seed(42)
        self.W = np.random.normal(0.0, 1, (self.n_input, self.n_output))
        self.bias = np.random.random(size=(self.n_output))

    def forward_pass_a(self, X):
      tmp = 0.0
      for i in range(len(X[0])):
          tmp += X[0][i] * self.W[i] 
      tmp += self.bias
      return tmp

#input_data = np.array([[1, 2, 3, 4, 5]])
#layer = Dense(3, n_input=5)

#output = layer.forward_pass_a(input_data)
#print(output)

class ReLU():
    def forward_pass(self, x):
        x[x<0] = 0
        return np.amax(x)

#activation = ReLU()
#asd = np.array([-21,-2])
#print(activation.forward_pass(output))